# Fall Detection System with YOLOv8 and State Machine

This repository implements a real-time fall detection system using the YOLOv8 object detection model and a state machine-based temporal logic. The application processes either local video files or RTSP streams, detects humans using YOLO, and determines fall events based on changes in bounding box orientation and time-based thresholds.

For any queries or contributions, feel free to reach out or open an issue.

## ğŸ“¦ Features

- ğŸ” Real-time human detection using YOLOv8 (Ultralytics)
- ğŸ¯ Fall detection logic based on bounding box geometry and state transitions
- ğŸ§  State machine for robust event tracking: `start â†’ watchdog â†’ generateEventCheck`
- ğŸ“¸ Video feed recording at full FPS and controlled FPS
- ğŸ“ Annotated video output with timestamps and bounding box metadata
- ğŸ“ Support for both local video files and RTSP streams

---

## ğŸ“ Project Structure

```text
â”œâ”€â”€ main.py                   # Entry point: captures video, handles state machine logic
â”œâ”€â”€ config.json              # Configuration for video source, model, and parameters
â”œâ”€â”€ perception_engine.py     # YOLOv8 inference engine and bounding box drawing
â”œâ”€â”€ analysis_engine.py       # (Optional expansion) Analysis logic or helper states
â”œâ”€â”€ helpers.py               # Utility functions for video metadata and state analysis
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ livefeed/                # Output folder for generated videos and metadata
````

---

## âš™ï¸ Installation

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/fall-detection-yolov8.git
cd fall-detection-yolov8
```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ§ª Configuration

Edit the `config.json` file to set:

```json
{
  "video": {
    "url": "D:/fall_detection_v2/fall.mp4",   // or RTSP stream
    "type": "local",                          // "local" or "rtsp"
    "fps": 24,
    "display": {
      "flag": true,
      "width": 640,
      "height": 360
    }
  },
  "perception_engine": {
    "model_name": "yolov8x.pt",
    "confidence_threshold": 0.75
  },
  "analysis_engine": {
    "state_tolerance_sec": 2,
    "fps": 2
  }
}
```

---

## ğŸš€ Usage

### Run the system:

```bash
python main.py
```

Press `q` at any time to quit the stream.

---

## ğŸ“‚ Outputs

After execution, youâ€™ll find the following outputs in the `livefeed/` directory:

* **`*_live_full_fps.mp4`** â€” Raw recording at source FPS
* **`*_live_feed_controlled_fps.mp4`** â€” Downsampled video for analysis
* **`*_annotated_controlled_fps.mp4`** â€” Bounding boxes and state annotations
* **`*_meta_data.json`** â€” Bounding box metadata
* **`*_events.json`** â€” All detected fall events with timestamps

---

## ğŸ§  How It Works

1. **YOLOv8** detects people in each frame.
2. Bounding box width and height ratios are monitored.
3. A state machine transitions through:

   * `start` â†’ waiting for bounding box flip (width > height to height > width)
   * `watchdog` â†’ monitoring for duration (`state_tolerance_sec`)
   * `generateEventCheck` â†’ confirming and logging fall events
4. Events are saved and annotated on the video.

---

## ğŸ› ï¸ Requirements

* Python 3.11
* NVIDIA GPU (recommended for YOLO inference)
* `yolov8x.pt` model from [Ultralytics](https://github.com/ultralytics/ultralytics)

---

## ğŸ“Œ TODO / Improvements

* Expand `analysis_engine.py` for modular event logic
* Add alert system (email, SMS)
* Add pose estimation or optical flow module
* Multi-person fall differentiation
* GUI for easier interaction

---

## ğŸ“„ License

This project is released under the MIT License.

---

## ğŸ‘¨â€ğŸ’» Author

Muhammad Ammar Anees, Asad Soomro

